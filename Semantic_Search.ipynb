{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic_Search.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mSIunOVtbPB"
      },
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install sentence-splitter\n",
        "!pip install transformers\n",
        "!pip install SentencePiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT7Ly5nLuXkI"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVU3Q3JTxmKn"
      },
      "source": [
        "import scipy\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx2_g6Bmuj_w"
      },
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz_qrYOqunhX"
      },
      "source": [
        "import json\n",
        "t = open('/content/ic.json')\n",
        "data = json.load(t)\n",
        "sentences = []\n",
        "responses = []\n",
        "for intents in data['intents']:\n",
        "  sentences.extend(intents['patterns'])\n",
        "  responses.extend(intents['responses'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnq9dF87vtM7"
      },
      "source": [
        "sentence_embeddings_base = model.encode(sentences)\n",
        "response_embeddings_base = model.encode(responses)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tolz_APXfKy9"
      },
      "source": [
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model_pegasus = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIjuL7G4gI1G"
      },
      "source": [
        "def get_response(input_text,num_return_sequences):\n",
        "  batch = tokenizer.prepare_seq2seq_batch([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
        "  translated = model_pegasus.generate(**batch,max_length=60,num_beams=10, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  return tgt_text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr5Ff3zft0Q6"
      },
      "source": [
        "from statistics import mode,mean"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgrCahWM433Y"
      },
      "source": [
        "def average(prob_min,prob_min_index):\n",
        "  avg = []\n",
        "  index = []\n",
        "  values = np.array(prob_min_index)\n",
        "  for i in set(prob_min_index):\n",
        "    a = []\n",
        "    t = np.where(values == i)[0]\n",
        "    for j in t:\n",
        "      a.append(prob_min[j])\n",
        "    avg.append(mean(a))\n",
        "    index.append(i)\n",
        "  return avg,index"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZMJkcJ4xvid"
      },
      "source": [
        "def prob(queries,query_embeddings,base):\n",
        "  prob_min = []\n",
        "  prob_min_index = []\n",
        "  for query, query_embedding in zip(queries, query_embeddings):\n",
        "      distances = scipy.spatial.distance.cdist([query_embedding], base, \"cosine\")[0]\n",
        "\n",
        "      prob_min.append(min(distances))\n",
        "      prob_min_index.append(list(distances).index(min(distances)))\n",
        "  return prob_min, prob_min_index"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-bgMWeaxm3p",
        "outputId": "ce432339-cc3f-4628-c4d1-0bc3cc9c2d69"
      },
      "source": [
        "query = input('What\\'s your query? : ')  \n",
        "queries = get_response(query, 10) \n",
        "query_embeddings = model.encode(queries)\n",
        "\n",
        "prob_min, prob_min_index = prob(queries,query_embeddings,sentence_embeddings_base)\n",
        "avg,index = average(prob_min,prob_min_index)\n",
        "\n",
        "if len(set([int(i*100) for i in avg])) == 1 and len([int(i*100) for i in avg]) != 1:\n",
        "  print('ok')\n",
        "  prob_min_resp, prob_min_index_resp = prob(queries,query_embeddings,response_embeddings_base)\n",
        "  avg_resp,index_resp = average(prob_min_resp,prob_min_index_resp)\n",
        "  final = index_resp[avg_resp.index(min(avg_resp))]+1\n",
        "else:\n",
        "  final = index[avg.index(min(avg))]+1\n",
        "\n",
        "for intents in data['intents']:\n",
        "  if intents['tag'] == 'q'+ str(final):\n",
        "    print(intents['responses'][0])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What's your query? : different paper for GATE or CS paper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:3365: FutureWarning: \n",
            "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
            "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
            "your targets.\n",
            "\n",
            "Here is a short example:\n",
            "\n",
            "model_inputs = tokenizer(src_texts, ...)\n",
            "with tokenizer.as_target_tokenizer():\n",
            "    labels = tokenizer(tgt_texts, ...)\n",
            "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
            "\n",
            "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
            "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
            "\n",
            "  warnings.warn(formatted_warning, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of now there is no separate paper for CCE in GATE exam, Students will have to write the paper of computer science. But for a student who is pursuing a Computer Science related degree, learning 1-2 courses more shouldnâ€™t pose a problem. Syllabus: (http://www.gate.iitg.ac.in)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a5mMTj4bR3m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}